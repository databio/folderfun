---
title: "Getting started with folderfun"
author: "Nathan Sheffield"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{1. Getting started with folderfun}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
# These settings make the vignette prettier
knitr::opts_chunk$set(results="hold", collapse=TRUE)
```

# Overview

The `folderfun` (short for *folder functions*) package makes it easy for you to manage files on disk for your R project.

In a basic R project, you'll probably want to read in data and write out plots or results. By default, the reading, plotting, and writing functions will read or write files in your current working directory. That might be fine for small, simple projects, but but it breaks down for many real-world use cases.

For example, what if you save your R project as a git repository (which is a good idea)? You don't want to store large, compressed input files in the same folder, nor do you want to commit your plot outputs. Instead, you'll want to store the data and results in other folders. Large projects can also require multiple folders for both input and output -- for example, you may load some shared data resource that lives in a group folder as well as some of your own project-specific resources. What if you want to work on a project with multiple people? These distributed folders can be organized in different ways and reside on different file systems in different computing environments. It can become a nightmare to keep track of the locations of all the folders on disk where different data and results are stored. And if you start hard-coding paths inside your R script, you make your code less portable, because it will only be able to be run in that computing environment. What if data changes locations? Your code breaks.

`folderfun` solves all these issues by making it dead simple to use *wrapper folder functions* to point to different data sources. Instead of pointing to input or output files with absolute file names, we define a function that remembers a root folder, and then use relative filenames with that function to identify individual files. Coupled with environment variables that define parent folder locations, you can easily maintain project-level subfolders with code that works across individuals and computing environments with almost no effort. This makes your code more portable and sharable and enables multiple users to work together on complex projects in different compute environments while sharing a single code base. Are you convinced yet?

## Motivation and basic use case

Let's say we have a project that needs to read data from one folder, let's call it `data`, and write results to another folder, let's call it `results`. Here's how you might start this analysis naively:

```{r, eval=FALSE}
# Load our data:
input1 = read.table("/long/and/annoying/path/to/hard/coded/file/data.txt")
input2 = read.table("/long/and/annoying/path/to/hard/coded/file/data2.txt")
output1 = processData(input)
output2 = processData2(input2)

# Run other analysis...

# Now write results:
write.table("/different/long/annoying/hard/coded/path/result.txt", output1)
write.table("/different/long/annoying/hard/coded/path/result2.txt", output2)
```

OK, that works... but this has multiple problems: First, you repeat the paths,  making it harder to change. Second, if the data move, you have to update multiple things. Third, this script won't work in a different compute environment since filepaths will almost certainly differ.

A better alternative would be to define a path variable beforehand, and then use it in multiple places:

```{r, eval=FALSE}
inputDir = "/long/and/annoying/path/to/hard/coded/file"
outputDir = "/different/long/annoying/hard/coded/path"

input1 = read.table(file.path(dataDir, "data.txt"))
input2 = read.table(file.path(dataDir, "data2.txt"))
output1 = processData(input)
output2 = processData2(input2)

# Run other analysis...

write.table(file.path(outputDir, "result.txt"))
write.table(file.path(outputDir, "result2.txt"))
```

That's much nicer; it limits the hard-coded folders to a single variable per folder, making them easier to maintain. Plus, now someone else could re-use this script by just adjusting the variable pointers at the top. But that `file.path(...)` syntax is really annoying; `folderfun` makes this nicer.


# Getting started with the `folderfun` approach

With `folderfun`, we'll use a function called `setff` to create functions, each of which will provide a path to a folder of interest. We assign each folder function a name (`In` and `Out` in this example), and provide the location to the folder:


```{r}
library(folderfun)
setff("In", "/long/and/annoying/path/to/hard/coded/file/")
setff("Out", "/different/long/annoying/hard/coded/path/")
```

These functions will create new functions named by prepending the text *ff* to our given name. By passing just a relative path, we can then reference any file contained within a folder for which we've created an accessor with `setff`, like this:

```{r}
ffIn("data.txt")
ffOut("result.txt")
```

So our original analysis would look something like this:

```{r, eval=FALSE}
input1 = read.table(ffIn("data.txt"))
input2 = read.table(ffIn("data2.txt"))
output1 = processData(input)
output2 = processData2(input2)

# Run other analysis...

write.table(ffOut("result.txt"))
write.table(ffOut("result2.txt"))
```

So, to reiterate: when you create a folder function named "In", you now have access to a function called `ffIn` that will access files in the directory referenced in the `setff` cakk. You can have as many folder functions you want with whatever names you like, as long as the names differ. Creating a function with a name already in use will overwrite the older function with that name.

## Using environment variables

Where `folderfun` becomes useful is if you want to use environment variables (or `R options`) to eliminate the step of hard-coding *anything* in your R session. For example, say we put this code into our `.bashrc` or `.profile` to define the locations for this compute environment:

```{bash, eval=FALSE}
export INDIR="/long/and/annoying/path/to/hard/coded/file/"
export OUTDIR="/different/long/annoying/hard/coded/path/"
```


```{r include=FALSE}
Sys.setenv(INDIR="/long/and/annoying/path/to/hard/coded/file/")
Sys.setenv(OUTDIR="/different/long/annoying/hard/coded/path/")
```

alternatively, to use `R options`:

```{r}
options(INDIR="/long/and/annoying/path/to/hard/coded/file/")
options(OUTDIR="/different/long/annoying/hard/coded/path/")
```
This is useful because it lets you create create project-level input and output folders that are sub folders of generic folders, and you can outsource the specification of the root directories to your `.profile`.

## Project-level 

So, assume we've set `INDIR` and `OUTDIR` as bash variables in `.bashrc` as presented above. Now all we need to do to establish project-specific sub directories is this:

```{r}
projName="myproject"

setff("In", file.path(Sys.getenv("INDIR", projName)))
setff("Out", file.path(Sys.getenv("OUTDIR", projName)))

ffIn()
ffIn("data.txt")
```

alternatively, using `R options`

```{r}
setff("In", file.path(getOption("INDIR"), projName))
setff("Out", file.path(getOption("OUTDIR"), projName))

ffIn()
ffIn("data.txt")
```

By moving specification of the folder root structure into an environment variable, we've made the whole project ultra-portable. This runs on any compute environment -- you just have to set the parent environment variables correctly. No absolute paths will need to be changed if data moves.

# Other usage style

`setff` may be used in a few different ways to achieve the same goal. The most straightforward way is as above, providing the function name and the path that it should return. Alternatively, the name of a current `option` or environment variable may be used, or the name of the function to create may be interpreted as an `option` or environment variable to look up for value.

```{r, eval=FALSE}
setff("RESOURCES", "/path/to/resources/folder")
setff("RESOURCES", pathVar="RESOURCES")
setff("RESOURCES")
```

With `RESOURCES` set--either as an option or an environment variable--to `"/path/to/resources/folder"`, each of those usage styles will have the same effect and return value, enabling `ffRESOURCES(...)` to be used to obtain an absolute path. Note that in the second case, the name for the function need not match the name of the `option` or environment variable.

In the most minimal pattern, e.g. `setff("RESOURCES")`, the name provided exactly determines the function name, but the strategy used to determine the path the function should reference can try a few different names. Succinctly, it favors `options` over environment variables, and first looks for a name exactly as given, trying an all-caps and then an all-lowercase version of the name until a nonempty value (neither `NULL` nor `""`) is found. So up to six names may be checked; if no match is found, the `setff` call will result in error.

# Management

You can get a list of all your loaded folder functions with the `listff` function:

```{r}
listff()
```

# Utilities

As noted, `setff` can attempt to find a path value for an `option` or environment variable name. To do so, it uses a function called `optOrEnvVar` within this package. This implementation of prioritized name resolution may be useful in other contexts, so it's independently available:

```{r, eval=FALSE}
name = "DUMMYTESTVAR"
value = "test_value"

optOrEnvVar(name)                 # NULL
Sys.setenv(name, value)
optOrEnvVar(name)                 # Now resolves
Sys.unsetenv(name)

optOrEnvVar(name)                 # NULL
optArg = list(value)
names(optArg) = name
options(optArg)
optOrEnvVar(name)                 # Now resolves

Sys.setenv(name, "new?")
optOrEnvVar(name)                 # on name collision, option trumps environment variable.
```

